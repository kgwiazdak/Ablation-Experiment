{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kgwiazdak/Ablation-Experiment/blob/main/Ablation%20Experiment\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment presents differences between ablations and shows when specified ablation type can break. I considered several heads types and 3 ablation types: zero, mean and resample. Than I compare zero and mean ablation with resample ablation in terms of several head types. As we know there is a backup mechanism and this way I'll find out whether all activations are different by the same percent. Recently optimal ablation came over, \"Optimal ablation for interpretability\", arXiv:2409.09951. It hasn't been considered within this collab as I met some obstackles with running their code but their work brings bright update to an ablation field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5TH2cXr8t-6"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b1Ci32htf5E"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfkJn_jirI8g",
    "outputId": "9d2b94ec-6948-4c8e-de34-c09a9f1b47fb"
   },
   "outputs": [],
   "source": [
    "%pip install transformer_lens\n",
    "%pip install circuitsvis\n",
    "%pip install ploty\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDgkhRuArPHZ"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import List, Optional, Union\n",
    "from transformer_lens.head_detector import get_supported_heads, detect_head\n",
    "import einops\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import torch\n",
    "# from circuitsvis.attention import attention_heads\n",
    "from fancy_einsum import einsum\n",
    "from IPython.display import HTML, IFrame\n",
    "from jaxtyping import Float\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens import ActivationCache, HookedTransformer\n",
    "\n",
    "\n",
    "from torch import Tensor\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")\n",
    "from jaxtyping import Int, Float\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3HF8JvQ6QWQZ",
    "outputId": "61ae1309-bc3d-4147-a169-eedb2b5882ce"
   },
   "outputs": [],
   "source": [
    "device: torch.device = utils.get_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qe9Xq6gUJ5eM"
   },
   "source": [
    "### Plot utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2hmQRgGxxeOV"
   },
   "outputs": [],
   "source": [
    "def imshow(tensor, **kwargs):\n",
    "    px.imshow(\n",
    "        utils.to_numpy(tensor),\n",
    "        color_continuous_midpoint=0.0,\n",
    "        color_continuous_scale=\"RdBu\",\n",
    "        **kwargs,\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZK58eBNoxTpt"
   },
   "source": [
    "### Setup model and prompt to all problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKoED3_VBdpp"
   },
   "source": [
    "The model that we will operate on is gpt2-small. The prompt that will be use in all next experiments is random generated tokens repeated 2 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BO9ignSLum3e",
    "outputId": "2ab43ff9-d85d-4960-a7aa-b6646fdb1922"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "device: torch.device = utils.get_device()\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLDJA6Y7xbBE"
   },
   "source": [
    "## Logit difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S2wO9bGZxfzi",
    "outputId": "10ef7d10-3548-4c83-fbf7-9bf73ca49f00"
   },
   "outputs": [],
   "source": [
    "prompt_format = [\n",
    "    \"When John and Mary went to the shops,{} gave the bag to\",\n",
    "    \"When Tom and James went to the park,{} gave the ball to\",\n",
    "    \"When Dan and Sid went to the shops,{} gave an apple to\",\n",
    "    \"After Martin and Amy went to the park,{} gave a drink to\",\n",
    "]\n",
    "names = [\n",
    "    (\" Mary\", \" John\"),\n",
    "    (\" Tom\", \" James\"),\n",
    "    (\" Dan\", \" Sid\"),\n",
    "    (\" Martin\", \" Amy\"),\n",
    "]\n",
    "# List of prompts\n",
    "prompts = []\n",
    "# List of answers, in the format (correct, incorrect)\n",
    "answers = []\n",
    "# List of the token (ie an integer) corresponding to each answer, in the format (correct_token, incorrect_token)\n",
    "answer_tokens = []\n",
    "for i in range(len(prompt_format)):\n",
    "    for j in range(2):\n",
    "        answers.append((names[i][j], names[i][1 - j]))\n",
    "        answer_tokens.append(\n",
    "            (\n",
    "                model.to_single_token(answers[-1][0]),\n",
    "                model.to_single_token(answers[-1][1]),\n",
    "            )\n",
    "        )\n",
    "        # Insert the *incorrect* answer to the prompt, making the correct answer the indirect object.\n",
    "        prompts.append(prompt_format[i].format(answers[-1][1]))\n",
    "answer_tokens = torch.tensor(answer_tokens).to(device)\n",
    "print(prompts)\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7esdbNTcxjf6"
   },
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(prompts, prepend_bos=True)\n",
    "original_logits, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6F_RkFaG010Z"
   },
   "outputs": [],
   "source": [
    "def logits_to_ave_logit_diff(logits, answer_tokens, per_prompt=False):\n",
    "    # Only the final logits are relevant for the answer\n",
    "    final_logits = logits[:, -1, :]\n",
    "    answer_logits = final_logits.gather(dim=-1, index=answer_tokens)\n",
    "    answer_logit_diff = answer_logits[:, 0] - answer_logits[:, 1]\n",
    "    if per_prompt:\n",
    "        return answer_logit_diff\n",
    "    else:\n",
    "        return answer_logit_diff.mean()\n",
    "\n",
    "original_average_logit_diff = logits_to_ave_logit_diff(original_logits, answer_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_z7ecoM7lGZ"
   },
   "source": [
    "## Activation Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P2_u5laz7oVk",
    "outputId": "65184431-3f6a-4b8c-d69a-ebf1b7a7f7cf"
   },
   "outputs": [],
   "source": [
    "corrupted_prompts = []\n",
    "for i in range(0, len(prompts), 2):\n",
    "    corrupted_prompts.append(prompts[i + 1])\n",
    "    corrupted_prompts.append(prompts[i])\n",
    "corrupted_tokens = model.to_tokens(corrupted_prompts, prepend_bos=True)\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(\n",
    "    corrupted_tokens, return_type=\"logits\"\n",
    ")\n",
    "corrupted_average_logit_diff = logits_to_ave_logit_diff(corrupted_logits, answer_tokens)\n",
    "print(\"Corrupted Average Logit Diff\", round(corrupted_average_logit_diff.item(), 2))\n",
    "print(\"Clean Average Logit Diff\", round(original_average_logit_diff.item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "AOxk-X_78xCd"
   },
   "outputs": [],
   "source": [
    "def normalize_patched_logit_diff(patched_logit_diff):\n",
    "    return (patched_logit_diff - corrupted_average_logit_diff) / (\n",
    "        original_average_logit_diff - corrupted_average_logit_diff\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rg0PsFegGaFq"
   },
   "source": [
    "#### Activation patching experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKxJoYNzWtbe"
   },
   "source": [
    "If you have only a little run you have to use one ablation method per run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AjSR38SwGj4j"
   },
   "outputs": [],
   "source": [
    "def patch_head_vector(\n",
    "    corrupted_head_vector,\n",
    "    hook,\n",
    "    head_index,\n",
    "    clean_cache,\n",
    "):\n",
    "    corrupted_head_vector[:, -1, head_index, :] = clean_cache[hook.name][\n",
    "        :, -1, head_index, :\n",
    "    ]\n",
    "    return corrupted_head_vector\n",
    "\n",
    "def zero_ablation_patch_head_vector(\n",
    "    corrupted_head_vector,\n",
    "    hook,\n",
    "    head_index,\n",
    "    clean_cache,\n",
    "):\n",
    "    corrupted_head_vector[:, -1, head_index, :] = 0\n",
    "    return corrupted_head_vector\n",
    "\n",
    "def mean_ablation_patch_head_vector(\n",
    "    corrupted_head_vector,\n",
    "    hook,\n",
    "    head_index,\n",
    "    clean_cache,\n",
    "):\n",
    "    corrupted_head_vector[:, -1, head_index, :] = clean_cache[hook.name][\n",
    "        :, -1, head_index, :\n",
    "    ].mean(dim=1, keepdim=True)\n",
    "    return corrupted_head_vector\n",
    "\n",
    "patch_head_vector_functions = [patch_head_vector, zero_ablation_patch_head_vector, mean_ablation_patch_head_vector]\n",
    "patch_head_vector_functions = [patch_head_vector, zero_ablation_patch_head_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgMaH87ZGrOd"
   },
   "outputs": [],
   "source": [
    "patched_head_z_diff = torch.zeros(\n",
    "    model.cfg.n_layers, model.cfg.n_heads, device=device, dtype=torch.float32\n",
    ")\n",
    "patched_head_zero_diff = torch.zeros(\n",
    "    model.cfg.n_layers, model.cfg.n_heads, device=device, dtype=torch.float32\n",
    ")\n",
    "patched_head_mean_diff = torch.zeros(\n",
    "    model.cfg.n_layers, model.cfg.n_heads, device=device, dtype=torch.float32\n",
    ")\n",
    "\n",
    "patched_head_diffs = [patched_head_z_diff, patched_head_zero_diff, patched_head_mean_diff]\n",
    "patched_head_diffs = [patched_head_z_diff, patched_head_zero_diff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "iEn8oMG27vHq"
   },
   "outputs": [],
   "source": [
    "for layer in range(model.cfg.n_layers):\n",
    "    for head_index in range(model.cfg.n_heads):\n",
    "        for function, diff in zip(patch_head_vector_functions, patched_head_diffs):\n",
    "          hook_fn = partial(function, head_index=head_index, clean_cache=cache)\n",
    "          patched_logits = model.run_with_hooks(\n",
    "              corrupted_tokens,\n",
    "              fwd_hooks=[(utils.get_act_name(\"z\", layer, \"attn\"), hook_fn)],\n",
    "              return_type=\"logits\",\n",
    "          )\n",
    "          patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)\n",
    "\n",
    "          diff[layer, head_index] = normalize_patched_logit_diff(\n",
    "              patched_logit_diff\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "dq1Xe-zL8vrg",
    "outputId": "f57348f3-cc3e-41a7-9d53-c2ceb9613f3e"
   },
   "outputs": [],
   "source": [
    "imshow(\n",
    "    patched_head_z_diff,\n",
    "    title=\"Logit Difference From Patched Head Output\",\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "uEV8EtjP_Rrg",
    "outputId": "d857d5aa-083f-400e-fd0d-e44abdeeeeeb"
   },
   "outputs": [],
   "source": [
    "imshow(\n",
    "    patched_head_zero_diff,\n",
    "    title=\"Logit Difference From Zero Ablated Patched Head Output\",\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "WGuWloWjNYAJ",
    "outputId": "d61ae083-9b4b-4a52-ca05-dd8d1e4f1231"
   },
   "outputs": [],
   "source": [
    "imshow(\n",
    "    patched_head_mean_diff,\n",
    "    title=\"Logit Difference From Mean Ablated Patched Head Output\",\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heads of specified types are hardcoded, full reasoning is contained in [Exploratory_Analysis_Demo.ipynb](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Exploratory_Analysis_Demo.ipynb#scrollTo=rJ59h9vXihax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Fc6bi5S_IFFC"
   },
   "outputs": [],
   "source": [
    "specified_heads = {\n",
    "    \"previous token heads\": [(2, 2), (4, 11)],\n",
    "    \"duplicate token heads\": [(0, 1), (3, 0), (0, 10)],\n",
    "    \"induction heads\": [(5, 5), (6,9), (5, 8), (5, 9)],\n",
    "    \"negative name mover heads\": [(10, 7), (11, 10)],\n",
    "    \"name mover heads\": [(9, 9), (9, 6), (10, 0)],\n",
    "    \"s-inhibition heads\": [(7, 3), (7, 9), (8, 6), (8, 10)],\n",
    "    \"backup name movers heads\": [(10, 10), (10, 6), (10, 2), (10, 1), (11, 2), (11, 9), (11, 3), (9, 7)]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I consider only heads where resample ablation produced score bigger than 0.05. That's due to calculation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "MwRusaqmaGoZ"
   },
   "outputs": [],
   "source": [
    "def count_procentage_difference(a, b):\n",
    "  c = torch.tensor([abs(bi/ai) for bi, ai in zip(b, a) if ai>0.05])\n",
    "  return int((c.sum()/len(a)*100).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the biggest change is observed in S-inhibition heads followed by name mover heads and name mover heads. This indicates that zero ablation and mean ablation is not good at finding S-inhibition heads and it also means that after zero ablating the S-inhibition head whole avg diff wouldn't change that much due to backup machanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LR0r7Je-kt5t",
    "outputId": "f41a3bd0-86a7-49bf-e443-f3256bbae77b"
   },
   "outputs": [],
   "source": [
    "for name, entries in specified_heads.items():\n",
    "  print(name)\n",
    "  r = []\n",
    "  z = []\n",
    "  m = []\n",
    "  for entry in entries:\n",
    "    r.append(patched_head_z_diff[entry].item())\n",
    "    z.append(patched_head_zero_diff[entry].item())\n",
    "    m.append(patched_head_mean_diff[entry].item())\n",
    "  r= torch.tensor(r)\n",
    "  z = torch.tensor(z)\n",
    "  m = torch.tensor(m)\n",
    "  print(f\"Procentage error between resample and zero ablation: {count_procentage_difference(r, z)}%\")\n",
    "  print(f\"Procentage error between resample and mean ablation: {count_procentage_difference(r, m)}%\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "216DvnHt4A-t"
   },
   "source": [
    "### Consideration about L1H10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxjTuFTIxBoz"
   },
   "source": [
    "Previous token heads, duplicate token heads, induction heads and negative name mover heads presented 0% differenced but it is partly because of my calculation method. I can see that heads L1H10 and L2H0 are active when using zero or mean output. L2H0 is one of less activated previous token heads. It's hard to determine what is L1H10 head. This head is too early to be negative name mover heads. Lower visualisation also determine that this head isn't previous token heads, duplicate token heads or induction heads.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "POcsiK8cy7vV"
   },
   "outputs": [],
   "source": [
    "seq_len = 100\n",
    "batch_size = 2\n",
    "original_tokens = torch.randint(\n",
    "    100, 20000, size=(batch_size, seq_len), device=\"cpu\"\n",
    ").to(device)\n",
    "repeated_tokens = einops.repeat(\n",
    "    original_tokens, \"batch seq_len -> batch (2 seq_len)\"\n",
    ").to(device)\n",
    "repeated_str = model.to_string(repeated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Svglb-73y0V-"
   },
   "outputs": [],
   "source": [
    "induction_heads = detect_head(model, repeated_str, \"induction_head\")\n",
    "previous_token_heads = detect_head(model, repeated_str, \"previous_token_head\")\n",
    "duplicate_token_heads = detect_head(model, repeated_str, \"duplicate_token_head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qUYSZpXYzm56",
    "outputId": "5283952c-ca71-4054-b635-18e38195df9c"
   },
   "outputs": [],
   "source": [
    "imshow(duplicate_token_heads, labels={\"x\": \"Head\", \"y\": \"Layer\"}, title=\"Duplicate Token Head Scores  (implementation)\")\n",
    "imshow(previous_token_heads, labels={\"x\": \"Head\", \"y\": \"Layer\"}, title=\"Previous Token Head Scores (implementation)\")\n",
    "imshow(induction_heads, labels={\"x\": \"Head\", \"y\": \"Layer\"}, title=\"Induction Head Scores (implementation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "XLSw2XFl3cHL"
   },
   "outputs": [],
   "source": [
    "def visualize_attention_patterns(\n",
    "    heads: Union[List[int], int, Float[torch.Tensor, \"heads\"]],\n",
    "    local_cache: ActivationCache,\n",
    "    local_tokens: torch.Tensor,\n",
    "    title: Optional[str] = \"\",\n",
    "    max_width: Optional[int] = 700,\n",
    ") -> str:\n",
    "    # If a single head is given, convert to a list\n",
    "    if isinstance(heads, int):\n",
    "        heads = [heads]\n",
    "\n",
    "    # Create the plotting data\n",
    "    labels: List[str] = []\n",
    "    patterns: List[Float[torch.Tensor, \"dest_pos src_pos\"]] = []\n",
    "\n",
    "    # Assume we have a single batch item\n",
    "    batch_index = 0\n",
    "\n",
    "    for head in heads:\n",
    "        # Set the label\n",
    "        layer = head // model.cfg.n_heads\n",
    "        head_index = head % model.cfg.n_heads\n",
    "        labels.append(f\"L{layer}H{head_index}\")\n",
    "\n",
    "        # Get the attention patterns for the head\n",
    "        # Attention patterns have shape [batch, head_index, query_pos, key_pos]\n",
    "        patterns.append(local_cache[\"attn\", layer][batch_index, head_index])\n",
    "\n",
    "    # Convert the tokens to strings (for the axis labels)\n",
    "    str_tokens = model.to_str_tokens(local_tokens)\n",
    "\n",
    "    # Combine the patterns into a single tensor\n",
    "    patterns: Float[torch.Tensor, \"head_index dest_pos src_pos\"] = torch.stack(\n",
    "        patterns, dim=0\n",
    "    )\n",
    "\n",
    "    # Circuitsvis Plot (note we get the code version so we can concatenate with the title)\n",
    "    plot = attention_heads(\n",
    "        attention=patterns, tokens=str_tokens, attention_head_names=labels\n",
    "    ).show_code()\n",
    "\n",
    "    # Display the title\n",
    "    title_html = f\"<h2>{title}</h2><br/>\"\n",
    "\n",
    "    # Return the visualisation as raw code\n",
    "    return f\"<div style='max-width: {str(max_width)}px;'>{title_html + plot}</div>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "5XJwT8VO4YDw"
   },
   "outputs": [],
   "source": [
    "repeated_tokens=repeated_tokens.flatten()\n",
    "_, repeated_cache = model.run_with_cache(repeated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VqdQ4lN121Lj",
    "outputId": "a344ccd0-b54c-4845-aba4-e7fb3da1bd32"
   },
   "outputs": [],
   "source": [
    "code = visualize_attention_patterns(\n",
    "    [59],\n",
    "    repeated_cache,\n",
    "    repeated_tokens,\n",
    "    title=\"Induction Heads\",\n",
    "    max_width=800,\n",
    ")\n",
    "HTML(code)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN356CJt3RLBuRi6zPcRFQv",
   "collapsed_sections": [
    "U5TH2cXr8t-6",
    "3b1Ci32htf5E",
    "LLDJA6Y7xbBE",
    "QAOPuBsdYmhC"
   ],
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
